---
title: "Practical 2"
author: "Osama Mahmoud and Berthold Lausen"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{practical2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Cluser analysis

K-means clustering aims to partition a set of $n$ observations with $p$ dimensional observations into k distinct and non-overlapping clusters. To do this, one has to specify k, the number of clusters. The idea is to estimate a clustering (partition) of the $n$ observations which has a within-cluster variation as small as possible.

We start by simulating data of two groups with bivariate normal distributed observations to illustrate the `k-means clustering` technique.

## K-means clustering

```{r}
set.seed(02022015)    # to 
x <- matrix(rnorm(50*2), ncol=2)
x[1:25,1] <- x[1:25,1]+3
x[1:25,2] <- x[1:25,2]-4
```

```{r}
library("ggplot2")
```

We can plot the simulated data
```{r}
(G.data    <- ggplot(as.data.frame(x), aes(V1,V2)) + geom_point(size=4))
```

We can use the function `kmeans` to group (i.e., cluster) observations into two groups
```{r}
my.km <- kmeans(x, centers=2, nstart=20)
my.km$cluster
```
Note that the parameter of `centers`, in the `kmeans` function above, is used to input the number of required clusters. Whilst the `nstart` parameter determines number of random sets to start with as a starting points for the clustering algorithm. *It is often recommended to try several random starts (`nstart` > 1) to get a better estimation*.

After calculating observations' assignment, we can now plot the obtained underlying clusters. Assignments of observations can be plotted next to data without assignment for illustration purpose.
```{r, results='hold'}
G.clust   <- ggplot(as.data.frame(x), aes(V1,V2)) + geom_point(color=my.km$cluster, size=4)
G.data; G.clust
```

If we try to cluster data into three groups, we can carry out a similar procedure
```{r}
my.km <- kmeans(x, centers=3, nstart=20)
my.km$cluster
```

```{r, results='hold'}
G.clust   <- ggplot(as.data.frame(x), aes(V1,V2)) + geom_point(color=my.km$cluster, size=4)
G.data; G.clust
```

Four clusters:
```{r}
my.km <- kmeans(x, centers=4, nstart=20)
my.km$cluster
```

```{r, results='hold'}
G.clust   <- ggplot(as.data.frame(x), aes(V1,V2)) + geom_point(color=my.km$cluster, size=4)
G.data; G.clust
```

Five clusters:
```{r}
my.km <- kmeans(x, centers=5, nstart=20)
my.km$cluster
```

```{r, results='hold'}
G.clust   <- ggplot(as.data.frame(x), aes(V1,V2)) + geom_point(color=my.km$cluster, size=4)
G.data; G.clust
```

## Hierarchical clustering

First, we will load in the `pottery` data set from the package `HSAUR2`
```{r, results='hide'}
data("pottery", package = "HSAUR2")
head(pottery)
dim(pottery)
```

It is recommended to normalise (centere and scale) its features

```{r}
s.pottery <- scale(pottery[,c(1:9)])    # normalise features
d.pottery <- dist(s.pottery)            # calculate distances between observations
```

## Agglomerative hierarchical clustering

Plot the fitted dendrogram based on the calculated Euclidean distances using the `complete linkage` method.

```{r, fig.width=7, fig.height=4}
plot(hclust(d.pottery, method="complete"),labels=(as.character(pottery$kiln)),
     hang=-1,cex=.75, main="",xlab="complete-linkage",ylab="level",sub="")
```

Plot dendrograms with various linkage methods

```{r, fig.width=7, fig.height=4}
plot(hclust(d.pottery, method="complete"),labels=(as.character(pottery$kiln)),
     hang=-1,cex=.5, main="",xlab="complete-linkage",ylab="level",sub="")
plot(hclust(d.pottery, method="single"),labels=(as.character(pottery$kiln)),
     hang=-1,cex=.5, main="",xlab="single-linkage",ylab="level",sub="")
plot(hclust(d.pottery, method="average"),labels=(as.character(pottery$kiln)),
     hang=-1,cex=.5, main="",xlab="average-linkage",ylab="level",sub="")
plot(hclust(d.pottery, method="centroid"),labels=(as.character(pottery$kiln)),
     hang=-1,cex=.5, main="",xlab="centroid-linkage",ylab="level",sub="")
```

Agglomerative hierarchical clustering with averageâ€“linkage using distance measure of $1 - r_{i,j}^2$

```{r, fig.width=7, fig.height=4}
d.pottery <- as.dist(1- cor(pottery[,c(1:9)])^2)
plot(hclust(d.pottery, method="average"),hang=-1,cex=.75,main="",xlab="average-linkage",ylab="level",sub="")
```

## Additive trees

Hierarchical clustering methods require objects (observations or clusters of observations) in same clusters to have identical distances to each other. In contrast, additive trees use the tree branch length to represent distances between objects. Allowing the within-cluster distances to vary among objects yields a tree diagram with varying branch lengths. Objects within a cluster can be compared by focusing on the vertical distance along the branches connecting them.

Plot an additive tree for the calculated Euclidean distances of the `pottery` data set

```{r, fig.width=7, fig.height=4}
library(clue)       # the package by which additive trees can be constructed
plot(ls_fit_addtree(d.pottery))
```
