---
title: "Practical 1"
author: "Osama Mahmoud and Berthold Lausen"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{practical1}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo=FALSE}
knitr::opts_chunk$set(comment = NA)
```

## Basic concepts

To get you familiar with underlying concepts of cluster analysis (unsupervised learning) and classification (supervised learning), we would use the `measure` data set, which is included within the `essexBigdata` package, to illustrate some basic concepts in statistics e.g. covariance, correlation and Euclidean distance.

### Covariance

The covariance of two random variables $X_{i}$ and $X_{j}$, denoted by $\sigma_{i,j}$, is a measure of linear dependence and defined by:

$$\sigma_{i,j} = Cov(X_{i}, X_{j}) = E[(X_{i} - \mu_{i})(X_{j} - \mu{j})],$$

with $\mu_{i} = E(X_{i})$ and $\mu_{j} = E(X_{j})$.

Note that, when $i = j$ we observe that the covariance $Cov(X_{i}, X_{i})$ is the variance of the
random variable $X_{i}$ which is denoted by $\sigma_{i}^2$ and can be defined as:

$$\sigma_{i}^2 = Cov(X_{i}, X_{i}) = E(X_{i} - \mu_{i})^2.$$

Larger absolute values of the covariance imply a higher degree of linear dependence between the two variables.

#### Covariance matrix

For q-dimensional multivariate random vectors $X = (X_{1}, ... , X_{q})$, we have $q$ variances $\sigma_{i}^2$ for $i = 1, ... , q$ and $\frac{q(q-1)}{2}$ covariances. The variances and covariances can be arranged in $q Ã— q$ symmetric matrix $\Sigma$ as follows:

$$\Sigma = 
\begin{pmatrix}
  \sigma_{1}^2 & \sigma_{1,2} & \cdots & \sigma_{1,q} \\
  \sigma_{2,1} & \sigma_{2}^2 & \cdots & \sigma_{2,q} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \sigma_{q,1} & \sigma_{q,2} & \cdots & \sigma_{q}^2
\end{pmatrix}$$

For a sample of $q$-dimensional observations, $x_{1}, ... , x_{n}$ with $x_{i} \in \mathbb{\mathbb{R}}^{q}$ for $i = 1, ... , n$, we can estimate the covariance matrix $\Sigma$ by estimating corresponding variances and covariances from the given sample.

We can use `R` to calculate the covariance matrix for the `measure` data set. To begin with, load the `measure` data

```{r results='hide', message=FALSE}
require(essexBigdata)
data(measure, package = "essexBigdata")
# Details of the 'measure' dataset can be shown by:
?measure
```

When loading a dataset in `R`, it is always a good idea to carry out a check. I tend to use:

```{r eval=FALSE}
head(measure)
dim(measure)
colnames(measure)
```

### Calculation of covariance matrix

Covariance matrix for measurements of the `measure` data can be calculated using:
```{r}
cov(measure[, c("chest", "waist", "hips")])
```

For observations of females only, we can use
```{r}
cov(subset(measure, gender == "female")[, c("chest", "waist", "hips")])
```

We can use the same concept to get the covariance matrix based on data for males only. **Could you write the corresponding `R` command to carry this out?** you should get the following result.
```{r echo=FALSE}
cov(subset(measure, gender == "male")[, c("chest", "waist", "hips")])
```

### Correlation

The covariance depends on the scale of each variable. Correlation is defined as a standardised covariance by the two standard deviations of the two
variables $X_{i}$ and $X_{j}$. Pearson's correlation coefficient is denoted by $\rho_{i,j}$ and defined by

$$\rho_{i,j} = \frac{Cov(X_{i}, X_{i})}{\sigma_{i} . \sigma_{j}},$$

where $\sigma_{i} = \sqrt{\sigma_{i}^2}$. Note that: $-1 \leq \rho_{i,j} \leq 1$

Correlation coefficients for measurements of the `measure` data can be calculated using:

```{r}
cor(measure[, c("chest", "waist", "hips")])
```

### Euclidean distance

Often we are interested to measure how similar (close) are the multivariate observations. For example, cluster analysis uses distance measures between the observations to group "similar" observations together. A common distance measure is the `Euclidean distance`:

$$d_{i,j} = \sqrt{\underset{k=1}{\overset{q}{\sum}}(x_{ik} - x_{jk})^2},$$

where $q$ is number of variables. While $x_{ik}$ and $x_{jk}$ are values of the $k$-th variable of observations $i$ and $j$ respectively.

Distance matrix for the observations of the `measure` data set can be calculated using `dist` function:

```{r}
# variables are normailsed (centered at mean and scaled by the standard deviations)
x <- dist(scale(measure[, c("chest", "waist", "hips")]))
as.dist(round(as.matrix(x), 2)[1:12, 1:12])
```
